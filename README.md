## ğŸš€ Autonomous Hackathon Judge

Multi-Agent Hackathon Scoring System powered by FastAPI, LangChain, and Model Context Protocol (MCP).
Judges GitHub repos, README pitches, and design heuristics, then aggregates into final scores with actionable feedback.

## ğŸ“Œ Features

Multi-Agent Architecture

Code Agent â†’ static analysis (docs, tests, CI/CD, structure).

Design Agent â†’ UX heuristics (README visuals, structure, accessibility).

Pitch Agent â†’ LLM-based scoring of README / pitch deck.

Judge Agent â†’ aggregates all agents with weights & prompts.

MCP Tooling

Connects to external MCP servers (@modelcontextprotocol/server-github, server-web) for GitHub + web context.

Async orchestration with LangChain Runnable Graph.

REST API Endpoints

POST /score â†’ detailed agent + final scoring.

POST /score_summary â†’ simplified verdict + feedback.

POST /score_profile â†’ aggregate scores across multiple repos for a GitHub user/org.

GET /debug/diag â†’ check GitHub rate limit + OpenAI key presence.

GET /health â†’ health check.

Optional Frontend

React/Vite UI (served via FastAPI static mount at /static).

Visual dashboards for scores, verdicts, and repo profiles.

## ğŸ— Architecture

```mermaid
flowchart TD
    A[Client / UI<br/>React / API / cURL] --> B[FastAPI<br/>app/main.py]

    B --> C1[Code Agent<br/>Heuristics: docs/tests/CI/license/structure]
    B --> C2[Design Agent<br/>Heuristics: UX, visuals, accessibility]
    B --> C3[Pitch Agent<br/>LLM via MCP<br/>README / Pitch Deck]

    C1 --> D[Judge Aggregator<br/>(Prompt + Weights)]
    C2 --> D
    C3 --> D

    D --> E[Final Score + Verdict<br/>JSON + UI Dashboard]

```

## âš¡ï¸ Quickstart
```
1. Clone repo
git clone https://github.com/ajithapamula/autojudge.git
cd autojudge
```
## 2. Create venv & install deps
```
python -m venv venv
venv\Scripts\activate   # Windows
# or source venv/bin/activate (Linux/Mac)

pip install -r requirements.txt
```
## 3. Set environment variables
```
Create .env:

OPENAI_API_KEY=sk-xxxxx
GITHUB_TOKEN=ghp_xxxxx       # optional, for higher rate limits
```
## 4. Run server
```
uvicorn app.main:app --reload --port 8080

API docs â†’ http://127.0.0.1:8080/docs
```
## ğŸ”¥ Example Usage
```
Health check
curl http://127.0.0.1:8080/health
# {"ok": true}
```
## Score single repo
```
curl -X POST "http://127.0.0.1:8080/score" \
     -H "Content-Type: application/json" \
     -d '{"repo_url":"https://github.com/ajithapamula/Edu-App"}'

Aggregate user profile
curl -X POST "http://127.0.0.1:8080/score_profile" \
     -H "Content-Type: application/json" \
     -d '{"handle":"ajithapamula","kind":"user","max_repos":5}'
```
## ğŸ§© Project Structure
```
app/
 â”œâ”€â”€ agents/          # Code, Design, Pitch, Judge agents
 â”œâ”€â”€ utils/           # MCP tools, GitHub client
 â”œâ”€â”€ graph.py         # LangChain graph orchestration
 â”œâ”€â”€ main.py          # FastAPI entrypoint
 â”œâ”€â”€ profile.py       # Profile aggregation
 â”œâ”€â”€ prompt.py        # Judge prompt template
 â””â”€â”€ frontend/        # (optional) React/Vite UI
```

## ğŸ›  Development
```
Run MCP servers locally
npx @modelcontextprotocol/server-github
npx @modelcontextprotocol/server-web
```
## Debug MCP tools
```
curl http://127.0.0.1:8080/debug/mcp-tools
```
## ğŸ“Š Roadmap
```
 Improve Pitch agent retry logic

 Add CI workflow for scoring self-tests

 Frontend dashboard (charts + repo comparisons)

 Persistent scoring DB (Postgres/Redis)

 Multi-modal design judging (images/screenshots)
```
 
 ## ğŸ§‘â€ğŸ’» Contributing
```
Fork the repo & create a feature branch.

Ensure tests pass:

pytest


Submit PR with clear description.
```
