ğŸš€ Autonomous Hackathon Judge

Multi-Agent Hackathon Scoring System powered by FastAPI, LangChain, and Model Context Protocol (MCP).
Judges GitHub repos, README pitches, and design heuristics, then aggregates into final scores with actionable feedback.

ğŸ“Œ Features

Multi-Agent Architecture

Code Agent â†’ static analysis (docs, tests, CI/CD, structure).

Design Agent â†’ UX heuristics (README visuals, structure, accessibility).

Pitch Agent â†’ LLM-based scoring of README / pitch deck.

Judge Agent â†’ aggregates all agents with weights & prompts.

MCP Tooling

Connects to external MCP servers (@modelcontextprotocol/server-github, server-web) for GitHub + web context.

Async orchestration with LangChain Runnable Graph.

REST API Endpoints

POST /score â†’ detailed agent + final scoring.

POST /score_summary â†’ simplified verdict + feedback.

POST /score_profile â†’ aggregate scores across multiple repos for a GitHub user/org.

GET /debug/diag â†’ check GitHub rate limit + OpenAI key presence.

GET /health â†’ health check.

Optional Frontend

React/Vite UI (served via FastAPI static mount at /static).

Visual dashboards for scores, verdicts, and repo profiles.

ğŸ— Architecture

               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚      Client / UI      â”‚
               â”‚ React / cURL / API    â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ REST
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚    FastAPI     â”‚
                   â”‚  app/main.py   â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                 â”‚                 â”‚
 â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Code Agent   â”‚   â”‚ Design Agent â”‚  â”‚ Pitch Agent  â”‚
 â”‚ Heuristics   â”‚   â”‚ Heuristics   â”‚  â”‚ LLM via MCP  â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                 â”‚                 â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼              â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Judge Aggregator   â”‚
                â”‚  (Prompt + Weights)  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
                   Final Score + Verdict

âš¡ï¸ Quickstart
1. Clone repo
git clone https://github.com/ajithapamula/autojudge.git
cd autojudge

2. Create venv & install deps

python -m venv venv
venv\Scripts\activate   # Windows
# or source venv/bin/activate (Linux/Mac)

pip install -r requirements.txt

3. Set environment variables

Create .env:

OPENAI_API_KEY=sk-xxxxx
GITHUB_TOKEN=ghp_xxxxx       # optional, for higher rate limits

4. Run server

uvicorn app.main:app --reload --port 8080

API docs â†’ http://127.0.0.1:8080/docs

ğŸ”¥ Example Usage
Health check
curl http://127.0.0.1:8080/health
# {"ok": true}

Score single repo
curl -X POST "http://127.0.0.1:8080/score" \
     -H "Content-Type: application/json" \
     -d '{"repo_url":"https://github.com/ajithapamula/Edu-App"}'

Aggregate user profile
curl -X POST "http://127.0.0.1:8080/score_profile" \
     -H "Content-Type: application/json" \
     -d '{"handle":"ajithapamula","kind":"user","max_repos":5}'

ğŸ§© Project Structure

app/
 â”œâ”€â”€ agents/          # Code, Design, Pitch, Judge agents
 â”œâ”€â”€ utils/           # MCP tools, GitHub client
 â”œâ”€â”€ graph.py         # LangChain graph orchestration
 â”œâ”€â”€ main.py          # FastAPI entrypoint
 â”œâ”€â”€ profile.py       # Profile aggregation
 â”œâ”€â”€ prompt.py        # Judge prompt template
 â””â”€â”€ frontend/        # (optional) React/Vite UI

ğŸ›  Development

Run MCP servers locally
npx @modelcontextprotocol/server-github
npx @modelcontextprotocol/server-web

Debug MCP tools
curl http://127.0.0.1:8080/debug/mcp-tools

ğŸ“Š Roadmap

 Improve Pitch agent retry logic

 Add CI workflow for scoring self-tests

 Frontend dashboard (charts + repo comparisons)

 Persistent scoring DB (Postgres/Redis)

 Multi-modal design judging (images/screenshots)

 ğŸ“Š Roadmap

 Improve Pitch agent retry logic

 Add CI workflow for scoring self-tests

 Frontend dashboard (charts + repo comparisons)

 Persistent scoring DB (Postgres/Redis)

 Multi-modal design judging (images/screenshots)


 ğŸ§‘â€ğŸ’» Contributing

Fork the repo & create a feature branch.

Ensure tests pass:

pytest


Submit PR with clear description.

